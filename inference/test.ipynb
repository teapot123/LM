{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaConfig, LlamaTokenizer, LlamaForCausalLM\n",
    "from model_utils import load_model, load_peft_model\n",
    "from chat_utils import read_dialogs_from_file, format_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs = read_dialogs_from_file('../data/trivia_qa/validation_1000_verb_1s_top1.json')\n",
    "model_name = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "model = load_model(model_name, quantization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\",})\n",
    "chats = format_tokens(dialogs, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = chats[0]\n",
    "tokens= torch.tensor(chat).long()\n",
    "tokens= tokens.unsqueeze(0)\n",
    "tokens2 = tokens.repeat([2,1])\n",
    "tokens2= tokens2.to(\"cuda:0\")\n",
    "outputs = model.generate(tokens2)\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True, )\n",
    "print(f\"Model output:\\n{output_text}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
